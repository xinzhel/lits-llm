{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from mem0 import Memory\n",
    "from mem0.memory.main import _build_filters_and_metadata, get_update_memory_messages\n",
    "from mem0.memory.utils import (\n",
    "    extract_json,\n",
    "    get_fact_retrieval_messages,\n",
    "    parse_messages,\n",
    "    parse_vision_messages,\n",
    "    process_telemetry_filters,\n",
    "    remove_code_blocks,\n",
    ")\n",
    "from mem0.configs.base import MemoryConfig\n",
    "from mem0.memory.telemetry import capture_event\n",
    "import concurrent\n",
    "from mem0.utils.factory import (\n",
    "    EmbedderFactory,\n",
    "    GraphStoreFactory,\n",
    "    LlmFactory,\n",
    "    VectorStoreFactory,\n",
    "    RerankerFactory,\n",
    ")\n",
    "from mem0.llms.aws_bedrock import AWSBedrockLLM\n",
    "from mem0.vector_stores.qdrant import Qdrant\n",
    "from mem0.configs.base import MemoryConfig\n",
    "from mem0.configs.vector_stores.qdrant import QdrantConfig\n",
    "from mem0.configs.vector_stores.s3_vectors import S3VectorsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8960e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access key: ASIAYCJJUI6MHMZCTOS7\n",
      "Session token: IQoJb3JpZ2luX2VjEDAaDmFwLXNvdXRoZWFzdC0yIkcwRQIgPGuv6wEj7q+A6DRct2txGQu/u/TWP3SCZQ0Ri9z/EEcCIQCdGsi0w1D6xU3ULAsUa5sF9eahYylpEwyLNiVtTO6hPyqTAwj5//////////8BEAAaDDU1NDY3NDk2NDM3NiIMZMCV2eNBWL1XtpvXKucCdJAm/K70wL9ycy8MTgljSDhc9/QruZX1WYEN0zhiitowyYdTScPHc73Zdh8Bkyed5EBqujuuWSVPNGY3l2qLqcgSds1Omjft6tkquWG6LjSRXLYYJ/AZdo9+qeRPw21yZs7u0TDGxs7nOSLDFUpddmbqlqIuGCF7gJvbsTGKKRzpHHzILlV9VA9DJmHVf+SJa+EDVkUY8M58HgBdHNR02NLWRv06aTU1E5MwpWT0qqufne+29SxSz6Uw7+JpKcujHm0OBW9m7qirEuFN7YDrFCmpAgdcsoKwSIs6CSwW3JU6ndD++x71A9JABy0Hi+KYdPRkHGJ7Vyh87C9l2B0klBL+bn4Td4ExbnvuWj/gVqGyFawGOVfGh6qkBn6aXknneGKcBoo+QUQKz2egKi3Yz4t7ZhAlLB+0OSTBSmUNkFMrJfc5V9CFfDenpigm2zMnIYJYNRn78ZUEMyYeUPd76s7zxOln66UwqtfEyAY6pAFt5PL97AkGiVAgNM3ugzbghhrJkiOYxhy5WXjXmkp7TeppuOhEsx6Oxr7w1B29JwEebwgRLM30+LJ2BsHl6ZyMA4fA3LpWzA2z3DeK6n0qGoGhuk2rgYANG27bs8PICVmsEoE4Bi7CqqMJL9KMX5oRpty6QjQfAygPKAvF1Tsbod/oo/3dXy2wHEP9begtm5aANXU5LO2urCJyOtJJlswSCustOQ==\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s = boto3.Session()\n",
    "creds = s.get_credentials().get_frozen_credentials()\n",
    "print(\"Access key:\", creds.access_key)\n",
    "print(\"Session token:\", creds.token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d02282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinzheli/miniconda3/envs/hf_transformers/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "config = MemoryConfig()\n",
    "# embedder configuration\n",
    "config.embedder.provider = \"huggingface\"\n",
    "config.embedder.config = {\"model\": \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"}\n",
    "\n",
    "# vector store configuration\n",
    "config.vector_store.provider = \"qdrant\"\n",
    "qdrant_config = QdrantConfig(\n",
    "    collection_name=\"mem0\",\n",
    "    embedding_model_dims=1536,\n",
    "    client=None,\n",
    "    host=None,\n",
    "    port=None,\n",
    "    path=\"/tmp/qdrant\",\n",
    "    url=None,\n",
    "    api_key=None,\n",
    "    on_disk=False\n",
    ")\n",
    "config.vector_store.config = qdrant_config\n",
    "\n",
    "# llm configuration\n",
    "# config.llm.provider = \"vllm\"\n",
    "# config.llm.config = {\"model\": \"Qwen/Qwen3-0.6B\", \"temperature\": 0.2, \"max_tokens\": 1024}\n",
    "config.llm.provider = \"aws_bedrock\"\n",
    "config.llm.config = {\"model\": \"anthropic.claude-3-5-haiku-20241022-v1:0\", \"temperature\": 0.2, \"max_tokens\": 2000}\n",
    "\n",
    "# create memory instance\n",
    "memory = Memory(config=config)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n",
    "]\n",
    "metadata ={\"category\": \"movie_recommendations\"}\n",
    "# result = memory.add(messages, user_id=\"alice\", metadata=metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d85dde",
   "metadata": {},
   "source": [
    "`memory.__init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0.llms.vllm import VllmLLM\n",
    "config.graph_store.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ab610",
   "metadata": {},
   "source": [
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.llm.config = {\"model\": \"NousResearch/Hermes-3-Lla.2-3B\", \"temperature\": 0.2, \"max_tokens\": 1024}\n",
    "llm = LlmFactory.create(config.llm.provider, config.llm.config)\n",
    "\n",
    "# import litellm\n",
    "# litellm.supports_function_calling(model=\"huggingface/NousResearch/Hermes-4-405B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875e5ce",
   "metadata": {},
   "source": [
    "Create embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652a7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface\n",
      "{'model': 'sentence-transformers/multi-qa-mpnet-base-cos-v1'}\n"
     ]
    }
   ],
   "source": [
    "print(config.embedder.provider)\n",
    "print(config.embedder.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f66205",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MemoryConfig()\n",
    "self.embedding_model = EmbedderFactory.create(\n",
    "            self.config.embedder.provider,\n",
    "            self.config.embedder.config,\n",
    "            self.config.vector_store.config,\n",
    "        )\n",
    "\n",
    "# How to customize the behaviour in the Memory.__init__ to do the following?\n",
    "# EmbedderFactory.create(provider_name=\"huggingface\",  config=embedder_config, vector_config=config.vector_store.config)\n",
    "\n",
    "# Here is the solution to replace the default OpenAI embedder with HuggingFace embedder\n",
    "# config = MemoryConfig()\n",
    "# config.embedder.provider = \"huggingface\"\n",
    "# config.embedder.config = {\"model\": \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"}\n",
    "\n",
    "from mem0.embeddings.openai import OpenAIEmbedding\n",
    "from mem0.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fb103",
   "metadata": {},
   "source": [
    "Create vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b61cac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qdrant\n",
      "collection_name='mem0' embedding_model_dims=1536 client=None host=None port=None path='/tmp/qdrant' url=None api_key=None on_disk=False\n"
     ]
    }
   ],
   "source": [
    "print(config.vector_store.provider)\n",
    "print(config.vector_store.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QdrantConfig(collection_name='mem0', embedding_model_dims=1536, client=None, host=None, port=None, path='/tmp/qdrant', url=None, api_key=None, on_disk=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mem0.vector_stores.qdrant import Qdrant\n",
    "\n",
    "qdrant_config = QdrantConfig(\n",
    "    collection_name=\"mem0\",\n",
    "    embedding_model_dims=1536,\n",
    "    client=None,\n",
    "    host=None,\n",
    "    port=None,\n",
    "    path=\"/tmp/qdrant\",\n",
    "    url=None,\n",
    "    api_key=None,\n",
    "    on_disk=False\n",
    ")\n",
    "config.vector_store.provider = \"qdrant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32bcd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mem0.vector_stores.qdrant.Qdrant object at 0x17ddd37a0>\n",
      "<qdrant_client.qdrant_client.QdrantClient object at 0x17df111f0>\n",
      "<qdrant_client.local.qdrant_local.QdrantLocal object at 0x17dfb8cb0>\n",
      "/tmp/qdrant\n"
     ]
    }
   ],
   "source": [
    "print(memory.vector_store)\n",
    "print(memory.vector_store.client)\n",
    "print(memory.vector_store.client._client)\n",
    "print(memory.vector_store.client._client.location) # QdrantLocal\n",
    "\n",
    "# print(memory.vector_store.client._client._api_key) # QdrantClient\n",
    "# print(memory.vector_store.client._client._host) # QdrantClient\n",
    "# print(memory.vector_store.client._client._port) # QdrantClient\n",
    "# print(memory.vector_store.client._client.rest_uri) # QdrantClient\n",
    "# print(memory.vector_store.client._client._host) # QdrantClient\n",
    "# print(memory.vector_store.client._client._port) # QdrantClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8622d86",
   "metadata": {},
   "source": [
    "`memory.add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b808c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(config.llm.config.get(\"enable_vision\"))\n",
    "print(memory.enable_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42bc00a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'alice'}\n",
      "{'user_id': 'alice'}\n"
     ]
    }
   ],
   "source": [
    "processed_metadata, effective_filters = _build_filters_and_metadata(\n",
    "    user_id=\"alice\",\n",
    "    agent_id=None,\n",
    "    run_id=None,\n",
    "    input_metadata=None,\n",
    ")\n",
    "print(processed_metadata)\n",
    "print(effective_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec9552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"I'm planning to watch a movie tonight. Any recommendations?\"}\n",
      "{'role': 'assistant', 'content': 'How about thriller movies? They can be quite engaging.'}\n",
      "{'role': 'user', 'content': \"I'm not a big fan of thriller movies but I love sci-fi movies.\"}\n",
      "{'role': 'assistant', 'content': \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n"
     ]
    }
   ],
   "source": [
    "messages = parse_vision_messages(messages)\n",
    "for message in messages:\n",
    "    print(message)\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c182bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future1 = executor.submit(memory._add_to_vector_store, messages, processed_metadata, effective_filters, infer=True)\n",
    "    future2 = [] # executor.submit(memory._add_to_graph, messages, effective_filters) # if memory.enable_graph is True\n",
    "\n",
    "    concurrent.futures.wait([future1, future2])\n",
    "\n",
    "    vector_store_result = future1.result()\n",
    "    graph_result = future2.result()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46816d24",
   "metadata": {},
   "source": [
    "`memory._add_to_vector_store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200b7015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(config.custom_fact_extraction_prompt)\n",
    "metadata ={\"category\": \"movie_recommendations\"}\n",
    "filters = effective_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8499a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Messages (type: <class 'str'>):\n",
      "user: I'm planning to watch a movie tonight. Any recommendations?\n",
      "assistant: How about thriller movies? They can be quite engaging.\n",
      "user: I'm not a big fan of thriller movies but I love sci-fi movies.\n",
      "assistant: Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "parsed_messages = parse_messages(messages)\n",
    "print(f\"Parsed Messages (type: {type(parsed_messages)}):\\n{parsed_messages} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02abd8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"facts\": [\"Loves sci-fi movies\", \"Not a big fan of thriller movies\"]}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.custom_fact_extraction_prompt:\n",
    "    system_prompt = config.custom_fact_extraction_prompt\n",
    "    user_prompt = f\"Input:\\n{parsed_messages}\"\n",
    "else:\n",
    "    # Determine if this should use agent memory extraction based on agent_id presence\n",
    "    # and role types in messages\n",
    "    is_agent_memory = memory._should_use_agent_memory_extraction(messages, metadata)\n",
    "    system_prompt, user_prompt = get_fact_retrieval_messages(parsed_messages, is_agent_memory)\n",
    "\n",
    "response = memory.llm.generate_response(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cfa8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"facts\": [\"Loves sci-fi movies\", \"Not a big fan of thriller movies\"]}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f91c0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Loves sci-fi movies', 'Not a big fan of thriller movies']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    response = remove_code_blocks(response)\n",
    "    if not response.strip():\n",
    "        new_retrieved_facts = []\n",
    "    else:\n",
    "        try:\n",
    "            # First try direct JSON parsing\n",
    "            new_retrieved_facts = json.loads(response)[\"facts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            # Try extracting JSON from response using built-in function\n",
    "            extracted_json = extract_json(response)\n",
    "            new_retrieved_facts = json.loads(extracted_json)[\"facts\"]\n",
    "except Exception as e:\n",
    "    # logger.error(f\"Error in new_retrieved_facts: {e}\")\n",
    "    new_retrieved_facts = []\n",
    "print(new_retrieved_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33a306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'alice'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if not new_retrieved_facts:\n",
    "#     logger.debug(\"No new facts retrieved from input. Skipping memory update LLM call.\")\n",
    "\n",
    "retrieved_old_memory = []\n",
    "new_message_embeddings = {}\n",
    "# Search for existing memories using the provided session identifiers\n",
    "# Use all available session identifiers for accurate memory retrieval\n",
    "search_filters = {}\n",
    "if filters.get(\"user_id\"):\n",
    "    search_filters[\"user_id\"] = filters[\"user_id\"]\n",
    "if filters.get(\"agent_id\"):\n",
    "    search_filters[\"agent_id\"] = filters[\"agent_id\"]\n",
    "if filters.get(\"run_id\"):\n",
    "    search_filters[\"run_id\"] = filters[\"run_id\"]\n",
    "print(search_filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce95fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.01s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "# for new_mem in new_retrieved_facts:\n",
    "new_mem = new_retrieved_facts[0]\n",
    "messages_embeddings = memory.embedding_model.embed(new_mem, \"add\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fd34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message_embeddings[new_mem] = messages_embeddings\n",
    "existing_memories = memory.vector_store.search(\n",
    "    query=new_mem,\n",
    "    vectors=messages_embeddings,\n",
    "    limit=5,\n",
    "    filters=search_filters,\n",
    ")\n",
    "\n",
    "\n",
    "for mem in existing_memories:\n",
    "    retrieved_old_memory.append({\"id\": mem.id, \"text\": mem.payload.get(\"data\", \"\")})\n",
    "unique_data = {}\n",
    "for item in retrieved_old_memory:\n",
    "    unique_data[item[\"id\"]] = item\n",
    "retrieved_old_memory = list(unique_data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logger.info(f\"Total existing memories: {len(retrieved_old_memory)}\")\n",
    "\n",
    "# mapping UUIDs with integers for handling UUID hallucinations\n",
    "temp_uuid_mapping = {}\n",
    "for idx, item in enumerate(retrieved_old_memory):\n",
    "    temp_uuid_mapping[str(idx)] = item[\"id\"]\n",
    "    retrieved_old_memory[idx][\"id\"] = str(idx)\n",
    "\n",
    "if new_retrieved_facts:\n",
    "    function_calling_prompt = get_update_memory_messages(\n",
    "        retrieved_old_memory, new_retrieved_facts, memory.config.custom_update_memory_prompt\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response: str = memory.llm.generate_response(\n",
    "            messages=[{\"role\": \"user\", \"content\": function_calling_prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in new memory actions response: {e}\")\n",
    "        response = \"\"\n",
    "    # response = \"\"\"{\n",
    "    #     \"memory\": [\n",
    "    #         {\n",
    "    #             \"id\": \"0\",\n",
    "    #             \"text\": \"Loves sci-fi movies\",\n",
    "    #             \"event\": \"ADD\"\n",
    "    #         },\n",
    "    #         {\n",
    "    #             \"id\": \"1\", \n",
    "    #             \"text\": \"Not a big fan of thriller movies\",\n",
    "    #             \"event\": \"ADD\"\n",
    "    #         }\n",
    "    #     ]\n",
    "    # }\n",
    "    # \"\"\"\n",
    "    try:\n",
    "        if not response or not response.strip():\n",
    "            logger.warning(\"Empty response from LLM, no memories to extract\")\n",
    "            new_memories_with_actions = {}\n",
    "        else:\n",
    "            response = remove_code_blocks(response)\n",
    "            new_memories_with_actions = json.loads(response)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Invalid JSON response: {e}\")\n",
    "        new_memories_with_actions = {}\n",
    "else:\n",
    "    new_memories_with_actions = {}\n",
    "\n",
    "returned_memories = []\n",
    "try:\n",
    "    for resp in new_memories_with_actions.get(\"memory\", []):\n",
    "        logger.info(resp)\n",
    "        try:\n",
    "            action_text = resp.get(\"text\")\n",
    "            if not action_text:\n",
    "                logger.info(\"Skipping memory entry because of empty `text` field.\")\n",
    "                continue\n",
    "\n",
    "            event_type = resp.get(\"event\")\n",
    "            if event_type == \"ADD\":\n",
    "                memory_id = memory._create_memory(\n",
    "                    data=action_text,\n",
    "                    existing_embeddings=new_message_embeddings,\n",
    "                    metadata=deepcopy(metadata),\n",
    "                )\n",
    "                returned_memories.append({\"id\": memory_id, \"memory\": action_text, \"event\": event_type})\n",
    "            elif event_type == \"UPDATE\":\n",
    "                memory._update_memory(\n",
    "                    memory_id=temp_uuid_mapping[resp.get(\"id\")],\n",
    "                    data=action_text,\n",
    "                    existing_embeddings=new_message_embeddings,\n",
    "                    metadata=deepcopy(metadata),\n",
    "                )\n",
    "                returned_memories.append(\n",
    "                    {\n",
    "                        \"id\": temp_uuid_mapping[resp.get(\"id\")],\n",
    "                        \"memory\": action_text,\n",
    "                        \"event\": event_type,\n",
    "                        \"previous_memory\": resp.get(\"old_memory\"),\n",
    "                    }\n",
    "                )\n",
    "            elif event_type == \"DELETE\":\n",
    "                memory._delete_memory(memory_id=temp_uuid_mapping[resp.get(\"id\")])\n",
    "                returned_memories.append(\n",
    "                    {\n",
    "                        \"id\": temp_uuid_mapping[resp.get(\"id\")],\n",
    "                        \"memory\": action_text,\n",
    "                        \"event\": event_type,\n",
    "                    }\n",
    "                )\n",
    "            elif event_type == \"NONE\":\n",
    "                # Even if content doesn't need updating, update session IDs if provided\n",
    "                memory_id = temp_uuid_mapping.get(resp.get(\"id\"))\n",
    "                if memory_id and (metadata.get(\"agent_id\") or metadata.get(\"run_id\")):\n",
    "                    # Update only the session identifiers, keep content the same\n",
    "                    existing_memory = memory.vector_store.get(vector_id=memory_id)\n",
    "                    updated_metadata = deepcopy(existing_memory.payload)\n",
    "                    if metadata.get(\"agent_id\"):\n",
    "                        updated_metadata[\"agent_id\"] = metadata[\"agent_id\"]\n",
    "                    if metadata.get(\"run_id\"):\n",
    "                        updated_metadata[\"run_id\"] = metadata[\"run_id\"]\n",
    "                    updated_metadata[\"updated_at\"] = datetime.now(pytz.timezone(\"US/Pacific\")).isoformat()\n",
    "\n",
    "                    memory.vector_store.update(\n",
    "                        vector_id=memory_id,\n",
    "                        vector=None,  # Keep same embeddings\n",
    "                        payload=updated_metadata,\n",
    "                    )\n",
    "                    logger.info(f\"Updated session IDs for memory {memory_id}\")\n",
    "                else:\n",
    "                    logger.info(\"NOOP for Memory.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing memory action: {resp}, Error: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error iterating new_memories_with_actions: {e}\")\n",
    "\n",
    "keys, encoded_ids = process_telemetry_filters(filters)\n",
    "capture_event(\n",
    "    \"mem0.add\",\n",
    "    memory,\n",
    "    {\"version\": memory.api_version, \"keys\": keys, \"encoded_ids\": encoded_ids, \"sync_type\": \"sync\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa226970",
   "metadata": {},
   "source": [
    "`memory.get_all(user_id=\"alice\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "_, effective_filters = _build_filters_and_metadata(\n",
    "    user_id=\"alice\", agent_id=None, run_id=None, input_filters=None\n",
    ")\n",
    "if not any(key in effective_filters for key in (\"user_id\", \"agent_id\", \"run_id\")):\n",
    "    raise ValueError(\"At least one of 'user_id', 'agent_id', or 'run_id' must be specified.\")\n",
    "\n",
    "keys, encoded_ids = process_telemetry_filters(effective_filters)\n",
    "capture_event(\n",
    "    \"mem0.get_all\", memory, {\"limit\": limit, \"keys\": keys, \"encoded_ids\": encoded_ids, \"sync_type\": \"sync\"}\n",
    ")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_memories = executor.submit(memory._get_all_from_vector_store, effective_filters, limit)\n",
    "    future_graph_entities = (\n",
    "        executor.submit(memory.graph.get_all, effective_filters, limit) if memory.enable_graph else None\n",
    "    )\n",
    "\n",
    "    concurrent.futures.wait(\n",
    "        [future_memories, future_graph_entities] if future_graph_entities else [future_memories]\n",
    "    )\n",
    "\n",
    "    all_memories_result = future_memories.result()\n",
    "    graph_entities_result = future_graph_entities.result() if future_graph_entities else None\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
